{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this one I'm hoping to graph out all the layers, hopefully to prove that all\n",
    "the trained models focus on a couple of specific features. We'll leave identifying\n",
    "the relevant features to maybe a different one? Or maybe it'll just happen near the end.\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from plotnine import *\n",
    "import plotnine\n",
    "from multiprocessing import Pool\n",
    "import statsmodels\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "working_dir = Path('/d/data/neis/neis_nn/output_7_newest')\n",
    "# input_file = working_dir / 'saved_model0.h5'\n",
    "data_file = working_dir / 'data.tsv'\n",
    "meta_file = working_dir / 'meta.tsv'\n",
    "id_file = working_dir / 'prefilter.tsv'\n",
    "out_file = working_dir / 'predicted_genes_full.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                7936      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 8,481\n",
      "Trainable params: 8,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_models = 5\n",
    "models = [tf.keras.models.load_model((working_dir / 'saved_model{0}.h5'.format(n))) for n in range(n_models)]\n",
    "    \n",
    "\n",
    "# model1 = tf.keras.models.load_model(input_file)\n",
    "data = np.loadtxt(data_file)\n",
    "meta = np.loadtxt(meta_file)\n",
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get linear models\n",
    "# from sklearn.linear_model import LassoLarsCV, LassoLars\n",
    "# model = LassoLars(alpha=0.001)\n",
    "# model.fit(data, meta.reshape((meta.shape[0],)))\n",
    "\n",
    "# abs_coefs = np.abs(model.coef_) * (np.sum(data, axis=0) > (data.shape[0] * 0.05))\n",
    "# sorted_index = np.argsort(abs_coefs)\n",
    "# good_idx = sorted_index[(-1 * int(np.count_nonzero(abs_coefs) * 0.2)):]\n",
    "\n",
    "# id_df = pd.read_csv(id_file, sep='\\t')\n",
    "# prediction_df = pd.DataFrame({'idx': good_idx,\n",
    "#                               'id': id_df.loc[good_idx, 'id'],\n",
    "#                               'average_impact': model.coef_[good_idx],\n",
    "#                               'abs_impact': abs_coefs[good_idx]})\n",
    "# prediction_df.set_index('idx', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(247, 32), (32,), (32, 16), (16,), (16, 1), (1,)]\n"
     ]
    }
   ],
   "source": [
    "weights = models[0].get_weights()\n",
    "print([x.shape for x in weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reshapeForPlot(matrix):\n",
    "#     #makes a df out of the matrix where each line is a point\n",
    "#     width, height = matrix.shape\n",
    "#     x_list = [x for x in range(width) for y in range(height)]\n",
    "#     y_list = [y for x in range(width) for y in range(height)]\n",
    "#     z_list = [matrix[x,y] for x in range(width) for y in range(height)]\n",
    "#     return pd.DataFrame({'features': x_list, 'neurons':y_list, 'val': z_list})\n",
    "\n",
    "# for x in range(0, len(weights), 2):\n",
    "#     layer_weights = reshapeForPlot(weights[x])\n",
    "#     bias_weights = pd.DataFrame({'neurons': range(weights[x+1].shape[0]), 'vals':weights[x+1]})\n",
    "    \n",
    "#     bias_text_y_delta = (np.max(weights[x+1]) - np.min(weights[x+1])) / 100\n",
    "#     bias_text_x_delta = weights[x+1].shape[0] / 60\n",
    "    \n",
    "#     layer_plot = (ggplot(layer_weights, aes('features', 'neurons', fill='val'))\n",
    "#                 + geom_tile(aes(width=.95, height=.95))\n",
    "#                 )\n",
    "#     bias_plot = (ggplot(bias_weights, aes('neurons', 'vals'))\n",
    "#              + geom_point()\n",
    "#              + geom_text(aes(label='neurons'), size=5, nudge_x=0.1, nudge_y=-0.01))\n",
    "    \n",
    "#     layer_plot.draw()\n",
    "#     layer_plot.save(working_dir / 'layer_{0}_weights.pdf'.format(int(np.ceil(x/2))), dpi=300, width=8, height=6, units='in')\n",
    "\n",
    "#     bias_plot.draw()\n",
    "#     bias_plot.save(working_dir / 'layer_{0}_bias.pdf'.format(int(np.ceil(x/2))), dpi=300, width=8, height=6, units='in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay we're just doing the prediction here\n",
    "#so the highest numbers in impact will be the selected vals\n",
    "\n",
    "def makeModSamples(sample):\n",
    "    n_features = sample.shape[0]\n",
    "    base = np.tile(sample, (n_features, 1)) #the same one repeated\n",
    "    msk = np.zeros_like(base)\n",
    "    for x in range(n_features):\n",
    "        if base[x][x] > 0:\n",
    "            base[x][x] = 0\n",
    "            msk[x,:] = 1\n",
    "\n",
    "    return np.array([base, msk]) #returns a 3D array\n",
    "    \n",
    "\n",
    "def getPredictions(model):\n",
    "    \n",
    "    def apply_fn(arr): #receives a 3D array generated by makeModSamples and reduces to two, then predict\n",
    "        preds = model.predict(arr[0])\n",
    "        return preds.reshape(-1,) * arr[1,:,0]\n",
    "\n",
    "    #here we are only counting ones above bound\n",
    "    def my_relu(n, bound):\n",
    "        if n > bound:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    #turn 1s to 0s\n",
    "    mod = np.apply_along_axis(makeModSamples, 1, data)\n",
    "    mod_preds = np.array([apply_fn(x) for x in mod]) #result should be a 2D array of samples x features\n",
    "    original_preds = model.predict(data)\n",
    "    counts = np.sum(data, axis=0)\n",
    "    \n",
    "#     print(original_preds, mod_preds)\n",
    "#     diffs = np.absolute(((mod_preds - original_preds) * (mod_preds > 0)))\n",
    "    #given that we have modded values, I'm going to try non-abs values.\n",
    "    diffs = (original_preds - mod_preds) * (np.absolute(mod_preds) > 0)\n",
    "    total_diffs = np.sum(diffs, axis=0)\n",
    "    avg_diffs = total_diffs / counts\n",
    "    \n",
    "#     print(diffs.shape, avg_diffs.shape)\n",
    "    \n",
    "    #we care about counts up to a point\n",
    "    \n",
    "    n_samples = data.shape[0]\n",
    "    bound = int(n_samples * 0.05) #that point being 5% of all samples\n",
    "#     bound = 5\n",
    "    count_modifier = np.array([my_relu(x, bound) for x in counts])\n",
    "    \n",
    "#     print(count_modifier.shape)\n",
    "    res = avg_diffs * count_modifier #this one gave the good results\n",
    "#     res = avg_diffs #here we're applying the count modifier later\n",
    "    \n",
    "#     res = np.sum(diffs, axis=0) * count_modifier\n",
    "#     print(count_modifier)\n",
    "\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this is an alternative method where we try to emulate the mistake we made before\n",
    "# #by just adding up the cases were a feature = 1\n",
    "\n",
    "# def makeModSamples(sample):\n",
    "#     n_features = sample.shape[0]\n",
    "#     base = np.tile(sample, (n_features, 1)) #the same one repeated\n",
    "#     msk = np.zeros_like(base)\n",
    "#     for x in range(n_features):\n",
    "#         if base[x][x] > 0:\n",
    "#             base[x][x] = 0\n",
    "#             msk[x,:] = 1\n",
    "\n",
    "#     return np.array([base, msk]) #returns a 3D array\n",
    "    \n",
    "\n",
    "# def getPredictions(model):\n",
    "    \n",
    "#     def apply_fn(arr): #receives a 3D array generated by makeModSamples and reduces to two, then predict\n",
    "#         preds = model.predict(arr[0])\n",
    "#         return preds.reshape(-1,) * arr[1,:,0]\n",
    "\n",
    "#     #here we are only counting ones above bound\n",
    "#     def my_relu(n, bound):\n",
    "#         if n > bound:\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "    \n",
    "#     #turn 1s to 0s\n",
    "# #     mod = np.apply_along_axis(makeModSamples, 1, data)\n",
    "# #     mod_preds = np.array([apply_fn(x) for x in mod]) #result should be a 2D array of samples x features\n",
    "#     original_preds = model.predict(data)\n",
    "#     counts = np.sum(data, axis=0)\n",
    "    \n",
    "# #     print(original_preds, mod_preds)\n",
    "# #     diffs = np.absolute(((mod_preds - original_preds) * (mod_preds > 0)))\n",
    "#     #given that we have modded values, I'm going to try non-abs values.\n",
    "# #     diffs = (original_preds - mod_preds) * data\n",
    "# #     total_diffs = np.sum(diffs, axis=0)\n",
    "# #     avg_diffs = total_diffs / counts\n",
    "\n",
    "#     n_samples = data.shape[0]\n",
    "#     bound = n_samples * 0.05 #that point being 5% of all samples\n",
    "# #     bound = 5\n",
    "#     count_modifier = np.sum(data, axis=0) > bound\n",
    "\n",
    "#     sums = np.sum((original_preds * data), axis=0) / counts * count_modifier\n",
    "    \n",
    "# #     sums = original_preds * (mod_pre)\n",
    "    \n",
    "#     return sums\n",
    "\n",
    "# # m = models[0]\n",
    "# # s = getPredictions(m)\n",
    "# # print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.loadtxt(data_file)\n",
    "# pred_res = getPredictions(model1)\n",
    "# print(np.argsort(pred_res)[:-10:-1])\n",
    "# # np.absolute((np.array([[1,2,3,4], [5,6,7,8]]) - np.array([9,9,9,9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.argsort(pred_res)[:-50:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_lists = []\n",
    "for m in models:\n",
    "    impact_lists.append(getPredictions(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lists = [np.absolute(x) for x in impact_lists]\n",
    "ind_lists = [np.argsort(x)[::-1] for x in abs_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hits = int(np.count_nonzero(impact_lists[0]) * 0.25) #we take top 20%\n",
    "top_hits = [np.sort(x[:n_hits]) for x in ind_lists]\n",
    "\n",
    "# #in this case we're saying top 20% of all hits, and more than 5% representation\n",
    "# n_min = data.shape[0] * 0.05\n",
    "# features_to_use = np.argwhere(np.sum(data, axis=0) > n_min).reshape(-1,)\n",
    "# top_hits_filtered = [np.intersect1d(hits, features_to_use) for hits in top_hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "50\n",
      "[ 61.   6.  69.  44.   7.   7.   4.  16.   7.  37.  12.   8.  29.   3.\n",
      "  16. 230.   3. 183.  89.  34.  56.  23.  56. 183. 203.  63.  53.  59.\n",
      "  56.  62.  50.  59. 153.  58. 196.  57.  70.  58.  57. 316.   3.  35.\n",
      "  57. 217.  58. 100.   4.  63.  64.  68.  63.  61.  59.  87. 162.  58.\n",
      "  33. 195.  33. 165.   6.  59.  42.   6.  38.  13.  56. 170.  61.  33.\n",
      " 114.  56.  77.  60.  53.  58.  85. 154. 269.   2.  54.  45.  44. 193.\n",
      "  52.  68.  54. 288.  56.  56.  61.  58.  35.  58.  61.  58. 287.  57.\n",
      "   3. 117.   6.  73.  32.   7.  25.  59. 217.   4.  16.   8.  20.   8.\n",
      "  60.  48.  53. 204. 130.  88.  43. 205.  82.  54.  57. 242.  60.  68.\n",
      "  32.  57. 153.  58.  59.  53.  40.  42.  14. 133.  48.  59.  63.  35.\n",
      "  81.  55.  55.  53.  56.  36.  34.  53. 176.  54.  54.  84.  54.  54.\n",
      "  84.  71.  74.  57.  56.  70.  56.  87.  58.  80.  16.  34.  25.  24.\n",
      "   6.  41.  33. 222.   3.  40.  52.   3. 189.  51.  53. 203.  57.  58.\n",
      "  66.  80.  23.  48.  46.  47.  35.  40.  98. 157.  34.   5.  90.  55.\n",
      " 132.  63.  59.  59.  55.  60.  57.  19.  27.   7.   4.  17.  38.  47.\n",
      "  35.  65.   3.  75.  54. 111.  15. 168.  91. 209.  72.  58.  53.  11.\n",
      "   7.  55.  79.  60.  55.  31.  53.  52.  52.  69.  24.  46.  10.   6.\n",
      "   8.  17.  14.   4. 171.   4.   4.  34.  29.]\n"
     ]
    }
   ],
   "source": [
    "# #We're looking for an intersection between three models\n",
    "common_inds = np.intersect1d(top_hits[0], top_hits[1])\n",
    "for x in top_hits[2:]:\n",
    "    common_inds = np.intersect1d(common_inds, x)\n",
    "common_inds = np.sort(common_inds)\n",
    "\n",
    "###DELETE AFTER\n",
    "# common_inds = [np.sort(x[:np.count_nonzero(impact_lists[0])]) for x in ind_lists][0]\n",
    "###\n",
    "# ind_list_sorted = np.matrix([np.sort(x) for x in ind_list], dtype=np.int32)\n",
    "# print(str(ind_list_sorted))\n",
    "\n",
    "average_impact = np.average([x[common_inds] for x in impact_lists], axis = 0)\n",
    "print(len(common_inds))\n",
    "print(n_hits)\n",
    "print(np.sum(data, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = pd.read_csv(id_file, sep='\\t')\n",
    "prediction_df = pd.DataFrame({'idx': common_inds,\n",
    "                              'id': id_df.loc[common_inds, 'id'],\n",
    "                              'average_impact': average_impact,\n",
    "                              'abs_impact': np.absolute(average_impact)})\n",
    "for x in range(n_models):\n",
    "    prediction_df['model_{0}'.format(x)] = impact_lists[x][common_inds]\n",
    "prediction_df.set_index('idx', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def doTTest(idx):\n",
    "#     test_df = pd.DataFrame({'x': data[:,idx], 'y': meta})\n",
    "#     lm = \n",
    "    \n",
    "#     return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(doTTest(5)['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>average_impact</th>\n",
       "      <th>abs_impact</th>\n",
       "      <th>model_0</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_3</th>\n",
       "      <th>model_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>NC_002946.2_ChrI:745000</td>\n",
       "      <td>0.017741</td>\n",
       "      <td>0.017741</td>\n",
       "      <td>0.019833</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.014598</td>\n",
       "      <td>0.021749</td>\n",
       "      <td>0.011626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>NC_002946.2_ChrI:747000</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>0.018917</td>\n",
       "      <td>0.019155</td>\n",
       "      <td>0.021768</td>\n",
       "      <td>0.017831</td>\n",
       "      <td>0.017004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>NC_002946.2_ChrI:1230000</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>0.018942</td>\n",
       "      <td>0.016301</td>\n",
       "      <td>0.020010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  average_impact  abs_impact   model_0   model_1  \\\n",
       "idx                                                                             \n",
       "82    NC_002946.2_ChrI:745000        0.017741    0.017741  0.019833  0.020898   \n",
       "84    NC_002946.2_ChrI:747000        0.018935    0.018935  0.018917  0.019155   \n",
       "130  NC_002946.2_ChrI:1230000        0.014278    0.014278  0.006490  0.009648   \n",
       "\n",
       "      model_2   model_3   model_4  \n",
       "idx                                \n",
       "82   0.014598  0.021749  0.011626  \n",
       "84   0.021768  0.017831  0.017004  \n",
       "130  0.018942  0.016301  0.020010  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #search for genes\n",
    "# def getChrCode(name):\n",
    "#     try:\n",
    "#         return re.search('_([0-9]+)_', name).group(1)\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# def makeReq(id):\n",
    "#     chr, start = id.split(':')\n",
    "#     end = int(start) + 5000\n",
    "#     if getChrCode(chr):\n",
    "#         url = url_template.format(getChrCode(chr), start, end)\n",
    "#         res = requests.get(url)\n",
    "#         try:\n",
    "# #             res_list = res.json()['response']['recordset']['records'][0]['fields'][1]['value']\n",
    "\n",
    "#             res_list = ['{0}:{1}'.format(x['id'].split('/')[0], str(x['fields'][1]['value'])) for x in res.json()['response']['recordset']['records']]\n",
    "#             res_str = ','.join(res_list)\n",
    "#             print(res_str)\n",
    "#             return res_str\n",
    "#         except Exception as e:\n",
    "#             print('error at this one ' + str(e) + ' ' + str(res.json()))\n",
    "#             return None\n",
    "\n",
    "#     else:\n",
    "#         print('bad chr ' + chr)\n",
    "    \n",
    "# import requests, re, json\n",
    "# url_template = 'https://plasmodb.org/plasmo/webservices/GeneQuestions/GenesByLocation.json?\\\n",
    "# organismSinglePick=Plasmodium falciparum 3D7&\\\n",
    "# chromosomeOptional={0}&\\\n",
    "# start_point={1}&\\\n",
    "# end_point={2}&\\\n",
    "# o-fields=gene_product,gene_name'\n",
    "\n",
    "# results = prediction_df['id'].map(makeReq)\n",
    "\n",
    "# # makeReq('Pf3D7_13_v3:1725000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is the by-genes mode\n",
    "# def makeGeneReq(id):\n",
    "#     gene_id = id.split(':')[0]\n",
    "#     if gene_id.endswith('_UTR'):\n",
    "#         gene_id = gene_id[:-4]\n",
    "        \n",
    "#     url = gene_url_template.format(gene_id)\n",
    "#     res = requests.get(url)\n",
    "#     try:\n",
    "# #             res_list = res.json()['response']['recordset']['records'][0]['fields'][1]['value']\n",
    "\n",
    "#         res_list = ['{0}:{1}'.format(id, str(x['fields'][0]['value'])) for x in res.json()['response']['recordset']['records']]\n",
    "#         res_str = ','.join(res_list)\n",
    "#         print(res_str)\n",
    "#         return res_str\n",
    "#     except Exception as e:\n",
    "#         print('error at this one ' + str(e) + ' ' + str(res.json()))\n",
    "#         return None\n",
    "\n",
    "\n",
    "# import requests, re, json\n",
    "# gene_url_template = 'https://plasmodb.org/plasmo/webservices/GeneQuestions/GeneByLocusTag.json?\\\n",
    "# ds_gene_ids_data={0}&\\\n",
    "# o-fields=gene_product'\n",
    "# results = prediction_df['id'].map(makeGeneReq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           id  average_impact    abs_impact   model_0  \\\n",
      "idx                                                                     \n",
      "178  NC_002946.2_ChrI:1570000    6.282706e-02  6.282706e-02  0.111023   \n",
      "122  NC_002946.2_ChrI:1122000    5.413979e-02  5.413979e-02  0.081052   \n",
      "232  NC_002946.2_ChrI:1950000   -4.933258e-02  4.933258e-02 -0.010399   \n",
      "17    NC_002946.2_ChrI:211000    4.247289e-02  4.247289e-02 -0.000768   \n",
      "182  NC_002946.2_ChrI:1596000    4.153582e-02  4.153582e-02  0.033582   \n",
      "..                        ...             ...           ...       ...   \n",
      "166  NC_002946.2_ChrI:1469000    2.698521e-04  2.698521e-04 -0.000989   \n",
      "85    NC_002946.2_ChrI:654000    1.455065e-04  1.455065e-04 -0.014168   \n",
      "158  NC_002946.2_ChrI:1459000   -1.007392e-04  1.007392e-04 -0.001024   \n",
      "248  NC_002946.2_ChrI:2035000   -3.063040e-05  3.063040e-05  0.000209   \n",
      "80    NC_002946.2_ChrI:515000   -9.752553e-08  9.752553e-08 -0.000438   \n",
      "\n",
      "      model_1   model_2   model_3   model_4  counts  \n",
      "idx                                                  \n",
      "178 -0.021054  0.020040  0.033529  0.170598    41.0  \n",
      "122  0.032294  0.017764  0.037367  0.102223    20.0  \n",
      "232 -0.055820 -0.035830 -0.051200 -0.093415    91.0  \n",
      "17   0.008809  0.006043  0.105038  0.093243    23.0  \n",
      "182  0.087630  0.011633  0.050051  0.024783    40.0  \n",
      "..        ...       ...       ...       ...     ...  \n",
      "166 -0.004920  0.000925 -0.006052  0.012385    56.0  \n",
      "85  -0.005345 -0.005407 -0.020242  0.045889   117.0  \n",
      "158 -0.001782 -0.005632  0.002048  0.005887    54.0  \n",
      "248  0.000990  0.000016  0.001422 -0.002790    52.0  \n",
      "80   0.002632  0.000002 -0.005626  0.003429    58.0  \n",
      "\n",
      "[213 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# prediction_df['genes'] = results\n",
    "prediction_df['counts'] = np.sum(data, axis=0)[[int(x) for x in prediction_df.index.to_numpy()]]\n",
    "prediction_df = prediction_df.sort_values('abs_impact', ascending=False)\n",
    "print(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(out_file, sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum1 = weights[0].sum(axis=1)\n",
    "# print(sum1.shape)\n",
    "# sum_df = pd.DataFrame({'sum':sum1})\n",
    "# sum_file = working_dir / 'sums.tsv'\n",
    "# sum_df.to_csv(sum_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# class fakemodel():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "#     def predict(self, arr):\n",
    "#         return np.sum(arr, axis=1).reshape(-1, 1)\n",
    "# data = np.array([[1,0,1,0], [0,0,0,1], [1,1,1,0]])\n",
    "# z = getPredictions(fakemodel())\n",
    "# for x in z:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  0,   2,   3,   9,  13,  14,  15,  16,  17,  18,  19,  20,  21,\n",
       "         22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,\n",
       "         35,  36,  37,  38,  39,  41,  43,  44,  45,  46,  47,  48,  49,\n",
       "         50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n",
       "         64,  65,  66,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  87,  88,  89,  90,  91,  92,\n",
       "         93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "        106, 107, 108, 111, 113, 115, 116, 118, 122, 124, 125, 126, 127,\n",
       "        128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "        142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169,\n",
       "        170, 171, 172, 173, 175, 178, 179, 180, 182, 184, 185, 187, 188,\n",
       "        189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
       "        202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "        216, 217, 221, 222, 223, 227, 228, 229, 231, 232, 233, 234, 235,\n",
       "        236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
       "        252, 259, 260, 262, 263, 264, 266, 267]),\n",
       " array([  0,   2,   3,   9,  13,  14,  15,  16,  17,  18,  19,  20,  21,\n",
       "         22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,\n",
       "         35,  36,  37,  38,  39,  41,  43,  44,  45,  46,  47,  48,  49,\n",
       "         50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n",
       "         64,  65,  66,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  87,  88,  89,  90,  91,  92,\n",
       "         93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "        106, 107, 108, 111, 113, 115, 116, 118, 122, 124, 125, 126, 127,\n",
       "        128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "        142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169,\n",
       "        170, 171, 172, 173, 175, 178, 179, 180, 182, 184, 185, 187, 188,\n",
       "        189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
       "        202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "        216, 217, 221, 222, 223, 227, 228, 229, 231, 232, 233, 234, 235,\n",
       "        236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
       "        252, 259, 260, 262, 263, 264, 266, 267]),\n",
       " array([  0,   2,   3,   9,  13,  14,  15,  16,  17,  18,  19,  20,  21,\n",
       "         22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  33,  34,  35,\n",
       "         36,  37,  38,  39,  41,  43,  44,  45,  46,  47,  48,  49,  50,\n",
       "         51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  62,  64,  65,\n",
       "         66,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,\n",
       "         81,  82,  83,  84,  85,  87,  88,  89,  90,  91,  92,  93,  94,\n",
       "         95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
       "        108, 111, 113, 116, 118, 122, 124, 125, 126, 127, 128, 129, 130,\n",
       "        131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144,\n",
       "        145, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158,\n",
       "        159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172,\n",
       "        173, 175, 178, 179, 180, 182, 183, 184, 185, 187, 188, 189, 190,\n",
       "        191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203,\n",
       "        204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216,\n",
       "        217, 221, 222, 223, 227, 228, 229, 231, 232, 233, 234, 235, 236,\n",
       "        237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 252,\n",
       "        258, 259, 260, 262, 263, 264, 266, 267]),\n",
       " array([  0,   2,   3,   9,  13,  14,  15,  16,  17,  18,  19,  20,  21,\n",
       "         22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,\n",
       "         35,  36,  37,  38,  39,  41,  43,  44,  45,  46,  47,  48,  49,\n",
       "         50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n",
       "         64,  65,  66,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  87,  88,  89,  90,  91,  92,\n",
       "         93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "        106, 107, 108, 111, 113, 115, 116, 118, 122, 124, 125, 126, 127,\n",
       "        128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "        142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169,\n",
       "        170, 171, 172, 173, 175, 178, 179, 180, 182, 184, 185, 187, 188,\n",
       "        189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
       "        202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "        216, 217, 221, 222, 223, 227, 228, 229, 231, 232, 233, 234, 235,\n",
       "        236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
       "        252, 259, 260, 262, 263, 264, 266, 267]),\n",
       " array([  0,   2,   3,   9,  13,  14,  15,  16,  17,  18,  19,  20,  21,\n",
       "         22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,\n",
       "         35,  36,  37,  38,  39,  41,  43,  44,  45,  46,  47,  48,  49,\n",
       "         50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n",
       "         64,  65,  66,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  87,  88,  89,  90,  91,  92,\n",
       "         93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "        106, 107, 108, 111, 113, 115, 116, 118, 122, 124, 125, 126, 127,\n",
       "        128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "        142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169,\n",
       "        170, 171, 172, 173, 175, 178, 179, 180, 182, 184, 185, 187, 188,\n",
       "        189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
       "        202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "        216, 217, 221, 222, 223, 227, 228, 229, 231, 232, 233, 234, 235,\n",
       "        236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
       "        252, 259, 260, 262, 263, 264, 266, 267])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
